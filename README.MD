This was done mostly as proof of concept / for a test. I highly recommend not using this and instead using the stackexchange API, the stack dump at archive.org, or making a different one.

This was a fun learning experience over how to treat HTML programatically, but it definitely does not feel useful and it is not perfect because the questions will get pushed forward as they get created making for inconsistencies or duplicates in the csv dataset. Perhaps a better scrape would be to fetch the link for each question ID, avoiding HTTP 301 for question ids that have permanently moved, but this would imply a much much larger strain on the servers (instead of 1 per 50 questions which can already feel quite abusive depending on the time between each req). As such, there's probably no non-abusive way to get such a large amount of info.